{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dm3R9Czr6sg3",
    "outputId": "80c57a9f-458e-4323-87c1-7bd745af1318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow version: 2.18.0\n",
      "üîç GPU Available: []\n",
      "üìÑ Loaded text with 1115394 characters and 65 unique characters.\n",
      "\n",
      "üöÄ Training for 1 epoch (fast mode)...\n",
      "Epoch 1/5\n",
      "683/683 [==============================] - 163s 234ms/step - loss: 2.3935\n",
      "Epoch 2/5\n",
      "683/683 [==============================] - 157s 230ms/step - loss: 1.8897\n",
      "Epoch 3/5\n",
      "683/683 [==============================] - 157s 230ms/step - loss: 1.7296\n",
      "Epoch 4/5\n",
      "683/683 [==============================] - 157s 229ms/step - loss: 1.6348\n",
      "Epoch 5/5\n",
      "683/683 [==============================] - 160s 234ms/step - loss: 1.5714\n",
      "‚úÖ Done training in 967.12 seconds.\n",
      "\n",
      "üìù Sample text:\n",
      "ROMEO: is he arpurd moud t.\n",
      "Thin ber.\n",
      "A: pmd fo'd tago 'ze shetumis:\n",
      "\n",
      "And ctr Is d areancurdan's tr s mel mORULUMIE ckis, sthe' th arweledeave our ose F; seburscher thicoune,\n",
      "ACI y a PAn iwaleryoulat whe:\n",
      "Whedin'd'st founknchit atr ild Squrist\n",
      "KAn\n",
      "\n",
      "Tward se.\n",
      "Gof ocubids eren.\n",
      "TIZAMang MO:\n",
      "ALCEThes's,\n",
      "Whe s\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ TensorFlow version:\", tf.__version__)\n",
    "print(\"üîç GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# 1. Load dataset\n",
    "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "text = open(path_to_file, 'rb').read().decode('utf-8')\n",
    "vocab = sorted(set(text))\n",
    "print(f\"üìÑ Loaded text with {len(text)} characters and {len(vocab)} unique characters.\")\n",
    "\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# 2. Create dataset\n",
    "seq_length = 50\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    return chunk[:-1], chunk[1:]\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# 3. Define smaller model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 256\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units)\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# 4. Train quickly\n",
    "EPOCHS = 5\n",
    "print(\"\\nüöÄ Training for 1 epoch (fast mode)...\")\n",
    "start = time.time()\n",
    "model.fit(dataset, epochs=EPOCHS)\n",
    "print(f\"‚úÖ Done training in {time.time() - start:.2f} seconds.\")\n",
    "\n",
    "# 5. Text generation\n",
    "def generate_text(model, start_string, temperature=1.0, num_generate=300):\n",
    "    input_eval = tf.expand_dims([char2idx[s] for s in start_string], 0)\n",
    "    text_generated = []\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = predictions[:, -1, :] / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "print(\"\\nüìù Sample text:\")\n",
    "print(generate_text(model, start_string=\"ROMEO: \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVrZcowD2Vot",
    "outputId": "6c043e7c-51fc-444c-dda6-95d75a0c03a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original Tokens: ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri', '.']\n",
      "2. Tokens Without Stopwords: ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri']\n",
      "3. Stemmed Words: ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources (only need to run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    # 1. Tokenize the sentence into words\n",
    "    tokens = word_tokenize(sentence)\n",
    "    print(\"1. Original Tokens:\", tokens)\n",
    "\n",
    "    # 2. Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words and word not in string.punctuation]\n",
    "    print(\"2. Tokens Without Stopwords:\", tokens_no_stopwords)\n",
    "\n",
    "    # 3. Apply stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens_no_stopwords]\n",
    "    print(\"3. Stemmed Words:\", stemmed_words)\n",
    "\n",
    "# Test the function\n",
    "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
    "preprocess_sentence(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZz6fDvTyti3",
    "outputId": "eb07c7e5-dab9-4c61-81ec-e38f3697d2ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "‚Ä¢ Text: Barack Obama | Label: PERSON | Start: 0 | End: 12\n",
      "‚Ä¢ Text: 44th | Label: ORDINAL | Start: 27 | End: 31\n",
      "‚Ä¢ Text: the United States | Label: GPE | Start: 45 | End: 62\n",
      "‚Ä¢ Text: the Nobel Peace Prize | Label: WORK_OF_ART | Start: 71 | End: 92\n",
      "‚Ä¢ Text: 2009 | Label: DATE | Start: 96 | End: 100\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract and print named entities\n",
    "print(\"Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"‚Ä¢ Text: {ent.text} | Label: {ent.label_} | Start: {ent.start_char} | End: {ent.end_char}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XjZJK_90ruN",
    "outputId": "65bc1823-ab1a-4782-8413-5afa01db835f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights matrix (after softmax):\n",
      "[[0.73105858 0.26894142]\n",
      " [0.26894142 0.73105858]]\n",
      "\n",
      "Final output matrix:\n",
      "[[2.07576569 3.07576569 4.07576569 5.07576569]\n",
      " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "    Q: Query matrix of shape (n_q, d)\n",
    "    K: Key matrix of shape (n_k, d)\n",
    "    V: Value matrix of shape (n_k, d_v)\n",
    "\n",
    "    Returns:\n",
    "    attention_weights: Softmax normalized attention weights matrix (n_q, n_k)\n",
    "    output: The final output matrix after applying attention (n_q, d_v)\n",
    "    \"\"\"\n",
    "    d = K.shape[1]  # key dimension\n",
    "\n",
    "    # 1. Dot product of Q and K·µÄ\n",
    "    scores = np.dot(Q, K.T)\n",
    "\n",
    "    # 2. Scale by sqrt(d)\n",
    "    scaled_scores = scores / np.sqrt(d)\n",
    "\n",
    "    # 3. Softmax on scaled scores along last axis (keys)\n",
    "    exp_scores = np.exp(scaled_scores - np.max(scaled_scores, axis=1, keepdims=True))  # for numerical stability\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # 4. Multiply attention weights by V\n",
    "    output = np.dot(attention_weights, V)\n",
    "\n",
    "    return attention_weights, output\n",
    "\n",
    "# Test input matrices\n",
    "Q = np.array([[1, 0, 1, 0],\n",
    "              [0, 1, 0, 1]])\n",
    "K = np.array([[1, 0, 1, 0],\n",
    "              [0, 1, 0, 1]])\n",
    "V = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8]])\n",
    "\n",
    "attention_weights, output = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Attention weights matrix (after softmax):\")\n",
    "print(attention_weights)\n",
    "print(\"\\nFinal output matrix:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "57e0c69b93b9469890fe688908ec2ca6",
      "af7a9685449e4443bee746890e670083",
      "8be81266d1034a40b1cb537ce6e8e2a0",
      "c4f465fe7bb747558e2b995512998cd3",
      "ac8666dee7614227bb9b5b6849ce20df",
      "e65309fc96fc4d91a0093be3f5c88d0a",
      "38737bc100a24aa1825d9812aab26798",
      "d70124094fb246b48957d7536d056edd",
      "bd48d726abbb46059cde9a37cdc940be",
      "bb1bd568b21347e8a48e7980c97be5ea",
      "124dc1807ad645049b1e27d1a6437fc9",
      "b544ae338bbe4cf7bfd645d772916cfb",
      "b70ca56fc8a745bd9a51279364e6df44",
      "a00becb9fca74d309336224862514b00",
      "340eb558df98429e937aba3392164b6d",
      "36ced870f277414a80e624c41edf0387",
      "ed832e247740443c9b057fbcf7844e62",
      "be2fd74d37c4423c8ad34aff338316a2",
      "0bf65efcb646406abf2410c18ff0377f",
      "c6f582f1819f4bc78519dd7e831ece83",
      "35820ad9fcc6489e9650a9437e0eb661",
      "a8de312d9fe8440fb71197efdff37204",
      "8cfa51fc149844f18d43aebd392224d4",
      "93a9c867521d47ac85060bcb9459b92e",
      "1d367492a2b544ec90d609de09d253e2",
      "65440ab3122b4263a8acec935938d90e",
      "172ec843c25847fcb65fac4bb2936d1b",
      "c9fc08501e6a4119b8602a3ee505f880",
      "888247214b4742e4b0a61b552b13e9bb",
      "52858139e938499e83441704f371fc75",
      "3727cc93d8624529bf7177ceb1a844f7",
      "f533b6bd0290444692ffd97473fd0085",
      "72746e0a1bfe4cf7a28c5d6de20afeec",
      "70cb36b5ac504acba9043fc4b604f2aa",
      "fedaa00eca4040eab59c4024828c7074",
      "d6176e502b514bde8f43da634f2d6efd",
      "0242cbd9d88842298c0102fd8e3e8968",
      "020da762bd34417fbb82c41b72fd11c3",
      "74faa9bd51ee455bbeece6f747d2e4ee",
      "7a503d2f0f364bd08228f47a63b99252",
      "56714b92f41d490e909772ea934b6199",
      "941aa489a9f94a95827d12a43190c333",
      "67243760cffa4d559aae816b5afb46dd",
      "a166434971a34c3dab784d1cee170f88"
     ]
    },
    "id": "Mu5enkdc0z9K",
    "outputId": "05be4ce4-fae8-4825-d8bb-f21e9e3822e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e0c69b93b9469890fe688908ec2ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b544ae338bbe4cf7bfd645d772916cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfa51fc149844f18d43aebd392224d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cb36b5ac504acba9043fc4b604f2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE\n",
      "Confidence Score: 0.9998\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained sentiment-analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
    "\n",
    "# Get prediction\n",
    "result = sentiment_analyzer(sentence)[0]\n",
    "\n",
    "# Print output\n",
    "print(f\"Sentiment: {result['label']}\")\n",
    "print(f\"Confidence Score: {result['score']:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
